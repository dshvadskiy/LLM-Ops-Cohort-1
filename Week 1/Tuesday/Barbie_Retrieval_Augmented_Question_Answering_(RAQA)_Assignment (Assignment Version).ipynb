{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dshvadskiy/LLM-Ops-Cohort-1/blob/main/Week%201/Tuesday/Barbie_Retrieval_Augmented_Question_Answering_(RAQA)_Assignment%20(Assignment%20Version).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questioning Barbie Reviews with RAQA (Retrieval Augmented Question Answering)\n",
        "\n",
        "In the following notebook, you are tasked with creating a system that answers questions based on information found in reviews of the 2023 Barbie movie.\n",
        "\n",
        "## Build 🏗️\n",
        "\n",
        "There are 3 main tasks in this notebook:\n",
        "\n",
        "1. Obtain and parse reviews from a review website\n",
        "2. Create a Vectorstore from the reviews\n",
        "3. Create a `RetrievalQA` using the VectorStore\n",
        "\n",
        "## Ship 🚢\n",
        "\n",
        "Create a Hugging Face Space that hosts your application.\n",
        "\n",
        "## Share 🚀\n",
        "\n",
        "Make a social media post about your final application."
      ],
      "metadata": {
        "id": "PLi09z_3qW8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">### Why RAQA and not RAG?\n",
        ">If we look at the original [paper](https://arxiv.org/abs/2005.11401), we find that RAG is a fairly specific and well defined term that isn't exactly the same as \"retrieve context, feed context to model in the prompt\".\n",
        ">For that reason, we're making the decision to delineate between \"actual\" RAG, and Retrieval Augmented Question Answering - which is not a well defined phrase."
      ],
      "metadata": {
        "id": "_BBraMxjrwOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-task Work\n",
        "\n",
        "All we really need to do to get started is to get our prerequisites!\n",
        "\n",
        "We'll be leveraging `langchain` and `openai` today.\n",
        "\n",
        "Check out the docs:\n",
        "- [LangChain](https://docs.langchain.com/docs/)\n",
        "- [OpenAI](https://github.com/openai/openai-python)"
      ],
      "metadata": {
        "id": "zcL8585DsZML"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQa6rjDLqPCI"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U openai langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Open AI API Key:\")"
      ],
      "metadata": {
        "id": "1wF8_ODX5h-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Data Preparation\n",
        "\n",
        "In this task we'll be collecting, and then parsing, our data."
      ],
      "metadata": {
        "id": "FLrL342RtWxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scraping IMDB Reviews of Barbie\n",
        "\n",
        "We'll use some Selenium based trickery to get the reviews we need to make our application.\n",
        "\n",
        "Check out the docs here:\n",
        "- [Selenium](https://www.selenium.dev/documentation/)"
      ],
      "metadata": {
        "id": "zh8QTtMhzY_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U requests"
      ],
      "metadata": {
        "id": "PsGPV2zHuCj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U scrapy selenium"
      ],
      "metadata": {
        "id": "uCNCZdzrw81B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install chromium-chromedriver"
      ],
      "metadata": {
        "id": "evZzjyX6xTbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scrapy.selector import Selector\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Niq6DKMOwPIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ],
      "metadata": {
        "id": "k9AA_f-DxGyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.imdb.com/title/tt1517268/reviews/?ref_=tt_ov_rt\"\n",
        "driver.get(url)"
      ],
      "metadata": {
        "id": "IsjYL7K_y1ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sel = Selector(text = driver.page_source)\n",
        "review_counts = sel.css('.lister .header span::text').extract_first().replace(',','').split(' ')[0]\n",
        "more_review_pages = int(int(review_counts)/25)"
      ],
      "metadata": {
        "id": "XPGysdwuy190"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(more_review_pages)):\n",
        "    try:\n",
        "        css_selector = 'load-more-trigger'\n",
        "        driver.find_element(By.ID, css_selector).click()\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "eGSJ0vuDy4D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_list = []\n",
        "review_date_list = []\n",
        "review_title_list = []\n",
        "author_list = []\n",
        "review_list = []\n",
        "review_url_list = []\n",
        "error_url_list = []\n",
        "error_msg_list = []\n",
        "reviews = driver.find_elements(By.CSS_SELECTOR, 'div.review-container')\n",
        "\n",
        "for d in tqdm(reviews):\n",
        "    try:\n",
        "        sel2 = Selector(text = d.get_attribute('innerHTML'))\n",
        "        try:\n",
        "            rating = sel2.css('.rating-other-user-rating span::text').extract_first()\n",
        "        except:\n",
        "            rating = np.NaN\n",
        "        try:\n",
        "            review = sel2.css('.text.show-more__control::text').extract_first()\n",
        "        except:\n",
        "            review = np.NaN\n",
        "        try:\n",
        "            review_date = sel2.css('.review-date::text').extract_first()\n",
        "        except:\n",
        "            review_date = np.NaN\n",
        "        try:\n",
        "            author = sel2.css('.display-name-link a::text').extract_first()\n",
        "        except:\n",
        "            author = np.NaN\n",
        "        try:\n",
        "            review_title = sel2.css('a.title::text').extract_first()\n",
        "        except:\n",
        "            review_title = np.NaN\n",
        "        try:\n",
        "            review_url = sel2.css('a.title::attr(href)').extract_first()\n",
        "        except:\n",
        "            review_url = np.NaN\n",
        "        rating_list.append(rating)\n",
        "        review_date_list.append(review_date)\n",
        "        review_title_list.append(review_title)\n",
        "        author_list.append(author)\n",
        "        review_list.append(review)\n",
        "        review_url_list.append(review_url)\n",
        "    except Exception as e:\n",
        "        error_url_list.append(url)\n",
        "        error_msg_list.append(e)\n",
        "review_df = pd.DataFrame({\n",
        "    'Review_Date':review_date_list,\n",
        "    'Author':author_list,\n",
        "    'Rating':rating_list,\n",
        "    'Review_Title':review_title_list,\n",
        "    'Review':review_list,\n",
        "    'Review_Url':review_url\n",
        "    })"
      ],
      "metadata": {
        "id": "c46cAciBxr2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df"
      ],
      "metadata": {
        "id": "W9A0C1tbyfuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save this `pd.DataFrame` as a `.csv` to our local session (this will be terminated when you terminate the Colab session) so we can leverage it in LangChain!\n",
        "\n",
        "Check out the docs if you get stuck:\n",
        "- [`to_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)"
      ],
      "metadata": {
        "id": "9gHLLYflzw_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.to_csv('reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "N9FDhwbNz64p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Parsing\n",
        "\n",
        "Now that we have our data - let's go ahead and start parsing it into a more usable format for LangChain!\n",
        "\n",
        "We'll be using the `CSVLoader` for this application.\n",
        "\n",
        "We also want to be sure to track the sources that our review came from!\n",
        "\n",
        "Check out the docs here:\n",
        "- [`CSVLoader`](https://python.langchain.com/docs/integrations/document_loaders/csv)"
      ],
      "metadata": {
        "id": "JtBD1H8ezNLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.csv_loader import CSVLoader"
      ],
      "metadata": {
        "id": "AdXub4CszAAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = CSVLoader(\n",
        "    file_path='reviews.csv',\n",
        "    source_column=\"Review\", metadata_columns=['Review_Date', 'Author', 'Rating', 'Review_Title', 'Review_Url'],\n",
        "    )\n",
        "\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "piE3_eSi0KGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0])\n",
        "print(len(data))"
      ],
      "metadata": {
        "id": "pvxnyjgh0TRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(data) == 150"
      ],
      "metadata": {
        "id": "26SZD9zb3luR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have collected our review information into a loader - we can go ahead and chunk the reviews into more manageable pieces.\n",
        "\n",
        "We'll be leveraging the `RecursiveCharacterTextSplitter` for this task today.\n",
        "\n",
        "While splitting our text seems like a simple enough task - getting this correct/incorrect can have massive downstream impacts on your application's performance.\n",
        "\n",
        "You can read the docs here:\n",
        "- [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter)\n",
        "\n",
        "We want to split our documents into 1000 character length chunks, with 100 characters of overlap.\n",
        "\n",
        "> ### HINT:\n",
        ">It's always worth it to check out the LangChain source code if you're ever in a bind - for instance, if you want to know how to transform a set of documents, check it out [here](https://github.com/langchain-ai/langchain/blob/5e9687a196410e9f41ebcd11eb3f2ca13925545b/libs/langchain/langchain/text_splitter.py#L268C18-L268C18)"
      ],
      "metadata": {
        "id": "_CTameZZ0r76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000, # the character length of the chunk\n",
        "    chunk_overlap = 50, # the character length of the overlap between chunks\n",
        "    length_function = len, # the length function\n",
        ")"
      ],
      "metadata": {
        "id": "uEgcUVtl00Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "y9RJUiUD2gS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documents))"
      ],
      "metadata": {
        "id": "w-oWu5XT3IG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(documents) == 184"
      ],
      "metadata": {
        "id": "6N4OXAhc3oIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our documents transformed into more manageable sizes, and with the correct metadata set-up, we're now ready to move on to creating our VectorStore!"
      ],
      "metadata": {
        "id": "ylT4jwmx3zCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Creating an \"Index\"\n",
        "\n",
        "The term \"index\" is used largely to mean: Structured documents parsed into a useful format for querying, retrieving, and use in the LLM application stack."
      ],
      "metadata": {
        "id": "9cB0L_CN38W5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Selecting Our VectorStore\n",
        "\n",
        "There are a number of different VectorStores, and a number of different strengths and weaknesses to each.\n",
        "\n",
        "In this notebook, we will be keeping it very simple by leveraging [Facebook AI Similarity Search](https://ai.meta.com/tools/faiss/#:~:text=FAISS%20(Facebook%20AI%20Similarity%20Search,more%20scalable%20similarity%20search%20functions.), or `FAISS`."
      ],
      "metadata": {
        "id": "GycdG53N4f9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U faiss-cpu tiktoken"
      ],
      "metadata": {
        "id": "T5o4vwSn4hfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to be setting up our VectorStore with the OpenAI embeddings model. While this embeddings model does not need to be consistent with the LLM selection, it does need to be consistent between embedding our index and embedding our queries over that index.\n",
        "\n",
        "While we don't have to worry too much about that in this example - it's something to keep in mind for more complex applications.\n",
        "\n",
        "We're going to leverage a [`CacheBackedEmbeddings`](https://python.langchain.com/docs/modules/data_connection/caching_embeddings )flow to prevent us from re-embedding similar queries over and over again.\n",
        "\n",
        "Not only will this save time, it will also save us precious embedding tokens, which will reduce the overall cost for our application.\n",
        "\n",
        ">#### Note:\n",
        ">The overall cost savings needs to be compared against the additional cost of storing the cached embeddings for a true cost/benefit analysis. If your users are submitting the same queries often, though, this pattern can be a massive reduction in cost."
      ],
      "metadata": {
        "id": "CGU96p5R54Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "\n",
        "byte_store = LocalFileStore(\"bytestore.db\")\n",
        "\n",
        "core_embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    core_embeddings_model,\n",
        "    document_embedding_cache=byte_store,\n",
        "    namespace=\"\"\n",
        ")\n",
        "\n",
        "vector_store = FAISS.from_documents(documents=documents,embedding=embedder)\n",
        ""
      ],
      "metadata": {
        "id": "AOzZWPU05WLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've created the VectorStore, we can check that it's working by embedding a query and retrieving passages from our reviews that are close to it."
      ],
      "metadata": {
        "id": "IGHzcE5i6fOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is Will Ferrell in this movie?\"\n",
        "embedding_vector = core_embeddings_model.embed_query(query)\n",
        "docs = vector_store.similarity_search_by_vector(embedding_vector, k = 4)\n",
        "\n",
        "for page in docs:\n",
        "  print(page.page_content)"
      ],
      "metadata": {
        "id": "JLOvFNxA6ZSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how much time the `CacheBackedEmbeddings` pattern saves us:"
      ],
      "metadata": {
        "id": "ix4fyOUS-fU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 1 -r 1\n",
        "query = \"I really wanted to enjoy this and I know that I am not the target audience but there were massive plot holes and no real flow.\"\n",
        "embedding_vector = core_embeddings_model.embed_query(query)\n",
        "docs = vector_store.similarity_search_by_vector(embedding_vector, k = 4)"
      ],
      "metadata": {
        "id": "eLXFKzu--m81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "query = \"I really wanted to enjoy this and I know that I am not the target audience but there were massive plot holes and no real flow.\"\n",
        "embedding_vector = core_embeddings_model.embed_query(query)\n",
        "docs = vector_store.similarity_search_by_vector(embedding_vector, k = 4)"
      ],
      "metadata": {
        "id": "pCwtsJbc_KPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, even over a significant number of runs - the cached query is significantly faster than the first instance of the query!\n",
        "\n",
        "With that, we're ready to move onto Task 3!"
      ],
      "metadata": {
        "id": "b4utL1EfARTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Building a Retrieval Chain\n",
        "\n",
        "In this task, we'll be making a Retrieval Chain which will allow us to ask semantic questions over our data.\n",
        "\n",
        "This part is rather abstracted away from us in LangChain and so it seems very powerful.\n",
        "\n",
        "Be sure to check the documentation, the source code, and other provided resources to build a deeper understanding of what's happening \"under the hood\"!"
      ],
      "metadata": {
        "id": "Po3kHNHGBp0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A Basic RetrievalQA Chain\n",
        "\n",
        "We're going to leverage `return_source_documents=True` to ensure we have proper sources for our reviews - should the end user want to verify the reviews themselves.\n",
        "\n",
        "Hallucinations [are](https://arxiv.org/abs/2202.03629) [a](https://arxiv.org/abs/2305.15852) [massive](https://arxiv.org/abs/2303.16104) [problem](https://arxiv.org/abs/2305.18248) in LLM applications.\n",
        "\n",
        "Though it has been tenuously shown that using Retrieval Augmentation [reduces hallucination in conversations](https://arxiv.org/pdf/2104.07567.pdf), one sure fire way to ensure your model is not hallucinating in a non-transparent way is to provide sources with your responses. This way the end-user can verify the output."
      ],
      "metadata": {
        "id": "i5Ux9XdzCDi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Our LLM\n",
        "\n",
        "In this notebook, we'll continue to leverage OpenAI's suite of models - this time we'll be using the `gpt-3.5-turbo` model to power our RetrievalQAWithSources chain.\n",
        "\n",
        "Check out the relevant documentation if you get stuck:\n",
        "- [`OpenAIChat()`](https://python.langchain.com/docs/modules/model_io/models/chat/)"
      ],
      "metadata": {
        "id": "O_waVo3AEk71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain-openai"
      ],
      "metadata": {
        "id": "o8qHDuQvF2u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0.01,\n",
        "    max_tokens=200,\n",
        ")"
      ],
      "metadata": {
        "id": "QfglvbBtExPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our chain.\n",
        "\n",
        "We'll need to make our VectorStore a retriever in order to be able to leverage it in our chain - let's check out the `as_retriever()` method.\n",
        "\n",
        "Relevant Documentation:\n",
        "- [`FAISS`](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html#langchain.vectorstores.faiss.FAISS.as_retriever)"
      ],
      "metadata": {
        "id": "w35ZkVEoE8II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "T8TWJMH8F_w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.callbacks import StdOutCallbackHandler\n",
        "\n",
        "handler = StdOutCallbackHandler()\n",
        "\n",
        "qa_with_sources_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    callbacks=[handler],\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "QeD8R6huFIf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_with_sources_chain({\"query\" : \"How was Will Ferrell in this movie?\"})"
      ],
      "metadata": {
        "id": "oqPaII9nF72R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_with_sources_chain({\"query\" : \"Do reviewers consider this movie Kenough?\"})"
      ],
      "metadata": {
        "id": "47Ov7N22MxOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And with that, we have our Barbie Review RAQA Application built!"
      ],
      "metadata": {
        "id": "QQVCkoy3H5Rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that we have our application ready, let's deploy it to a Hugging Face space with their Docker spaces.\n",
        "\n",
        "You can find the next part [here](https://ai-maker-space-barbie-raqa-application-chainlit-demo.hf.space/readme)!\n",
        "\n",
        "You've built, now it's time to ship! 🚢"
      ],
      "metadata": {
        "id": "KqIHum95dtns"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IASA1yaad6ml"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}